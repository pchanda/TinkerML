---
layout: post
title: Parameter learning and updates in simple word2vec
---

A lot of materials on word2vec models such as Skipgram and CBOW are available that explain the models really well. This post is just a drop in that ocean trying to clarify some of the points that I found useful in understanding the internals and explaining the models in line with the (almost the same) terminology used in the NLP lectures [Stanford ref].

We first consider the simple case of predicting one outside word (outside word is a different terminology used in the lecture specfying a context word) given a center word (the input word). The skip gram model in Figure 1 will learn the word embeddings while learning to predict a single outside word given a center word as input. Let $$\textbf{V} = \{w_1,w_2,...,w_{N_{\textbf{V}}} \}$$ be the vocabulary. Here $$N_{textbf{V}}$$ is the number of words in $$\textbf{V}$$.

Assume that the input center word occupies index $$c$$, i.e, $$w_c$$ in $$\textbf{V}$$. Also assume that the "correct" output "outside" (context) word occupies index $$o$$, i.e, $$w_o$$ in $$\textbf{V}$$. 

Figure 1 goes here

Let the input word is on-hot encoded as $$\textbf{x} = \{x_1,x_2,...,x_{N_{\textbf{V}}} \}$$. Let the dimension of the word embeddings be $$N$$. 

The model will learn two vector representations for each word represented by the two matrices $$\textbf{W}$$ and $$\hat{\textbf{W}}$$.
