---
layout: post
title: Parameter learning and updates in simple word2vec
---

A lot of materials on word2vec models such as Skipgram and CBOW are available that explain the models really well. This post is just a drop in that ocean trying to clarify some of the points that I found useful in understanding the internals and explaining the models in line with the (almost the same) terminology used in the NLP lectures [Stanford ref].

We first consider the simple case of predicting one outside word (outside word is a different terminology used in the lecture specfying a context word) given a center word (the input word). The skip gram model in Figure 1 will learn the word embeddings while learning to predict a single outside word given a center word as input. Let $$\textbf{V} = \{w_1,w_2,...,w_{N_{\textbf{V}}} \}$$ be the vocabulary. Here $$N_{\textbf{V}}$$ is the number of words in $$\textbf{V}$$.

Assume that the input center word occupies index $$c$$, i.e, $$w_c$$ in $$\textbf{V}$$. Also assume that the "correct" output "outside" (context) word occupies index $$o$$, i.e, $$w_o$$ in $$\textbf{V}$$. 

Figure 1 goes here

Let the input word is on-hot encoded as $$\textbf{x} = \{x_1,x_2,...,x_{N_{\textbf{V}}} \}$$. Let the dimension of the word embeddings be $$N$$. 

The model will learn two vector representations for each word represented by the two matrices $$\textbf{W}$$ and $$\hat{\textbf{W}}$$.
  
The matrix $$\textbf{W}$$ is the input-to-hidden layer weight matrix, has dimensions $$N \times N_{\textbf{V}}$$. 

Column $$j$$ of $$\textbf{W}$$ denoted by $$\textbf{v}_j$$ is the $$N$$ dimensional vector representation of $$j$$th word in the vocabulary ($$w_j$$) when the word is used as center word. So the vector representation of the input word $$w_c$$ is the $$c$$th column $$\textbf{v}_c$$ of $$\textbf{W}$$.

Similary, the matrix $$\hat{\textbf{W}}$$ is the hidden-to-output layer weight matrix, and also has dimensions $$N \times N_{\textbf{V}}$$. Column $$j$$ of $$\hat{\textbf{W}}$$ denoted by $$\hat{\textbf{v}}_j$$ is the $$N$$ dimensional vector representation of $$j^{th}$$ word in the vocabulary ($$w_j$$) when the word appears as outside (i.e context word) of another center word. 

Given the input $$\textbf{x}$$, the operation $$\textbf{Wx}$$ produces the $$N$$-dimensional vector $$\textbf{h}$$. That is, $$ \textbf{h} = \textbf{Wx}$$. But because $$\textbf{x}$$ is one-hot encoding of $$w_c$$, it has only the $$c$$th position as 1 (and rest are 0's), so $$\textbf{h}$$ is simply the $$c^{th}$$ column of $$\textbf{W}$$. That is $$ \textbf{h} = \textbf{Wx} = \textbf{v}_c$$ (column vector of dimension $$N$$). So this is simply pulling out the vector representation of the center word from that matrix $$\textbf{W}$$. 


From the hidden layer to the output layer, the other weight matrix $$\hat{\textbf{W}}$$ is used, which is also an $$N \times N_{\textbf{V}}$$ matrix.
Using these weights, we can compute a score $$u_j$$ for each word $$w_j$$ in the vocabulary:  $$u_j = {\hat{\textbf{v}}_j}^{T} \textbf{h} = {\hat{\textbf{v}}_j}^{T} \textbf{v}_c$$. That is, using matrix multiplication, $$ \textbf{u} = \hat{\textbf{W}}^T h$$, $$\textbf{u}$$ is a $$ N_{\textbf{V}} \times 1$$ column vector.

$$\exp( {\hat{\textbf{v}}_j}^{T} {\textbf{v}}_c ) $$.

$$ \sum_{t=1}^{N_{\textbf{V}}}  \exp({\hat{\textbf{v}}_t}^{T} {\textbf{v}}_c)   $$

$$ \dfrac {\exp( {\hat{\textbf{v}}_j}^{T} {\textbf{v}}_c ) } {\sum_{t=1}^{N_{\textbf{V}}}  \exp({\hat{\textbf{v}}_t}^{T} {\textbf{v}}_c)} $$

Next we convert the raw scores $$u_j$$ to probabilities using softmax,

$$ y_j  = \dfrac{\exp({u_j})}{\sum_{t=1}^{N_{\textbf{V}}} \exp(u_t)} 
 = \dfrac {\exp( {\hat{\textbf{v}}_j}^{T} {\textbf{v}}_c ) } {\sum_{t=1}^{N_{\textbf{V}}}  \exp({\hat{\textbf{v}}_t}^{T} {\textbf{v}}_c)} = P(w_j | w_c) $$


When $$j=o$$, the correct index of the outside word, 

$$ y_j = y_o $$

and 

$$ y_j = y_o = p( w_o | w_c)$$.  



