---
layout: post
title: Parameter learning and updates in simple word2vec
---

A lot of materials on word2vec models such as Skipgram and CBOW are available that explain the models really well. This post is just a drop in that ocean trying to clarify some of the points that I found useful in understanding the internals and explaining the models in line with the (almost the same) terminology used in the NLP lectures [Stanford ref].

We first consider the simple case of predicting one outside word (outside word is a different terminology used in the lecture specfying a context word) given a center word (the input word). The skip gram model in Figure 1 will learn the word embeddings while learning to predict a single outside word given a center word as input. Let $$\hat{\textbf{V}} = \{w_1,w_2,...,w_{N_{\hat{V}}} \}$$ be the vocabulary. Here $$N_{\hat{V}}$$ is the number of words in $$\hat{textbf{V}}$$.

Assume that the input center word occupies index $$c$$, i.e, $$w_c$$ in $$\hat{\textbf{V}}$$. Also assume that the "correct" output "outside" (context) word occupies index $$o$$, i.e, $$w_o$$ in $$\hat{\textbf{V}}$$. 

Figure 1 goes here

