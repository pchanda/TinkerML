{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "soviet-ceiling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "substantial-intranet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVa !\n",
      "Hi.\tSalut !\n"
     ]
    }
   ],
   "source": [
    "def read_data_nmt(data_path):\n",
    "    \"\"\"Load the English-French dataset.\"\"\"\n",
    "    with open(data_path, 'r', encoding = 'utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "raw_text = read_data_nmt('../data/fra-eng/fra.txt') #tab separated string\n",
    "print(raw_text[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-length",
   "metadata": {},
   "source": [
    "After downloading the dataset, we proceed with several preprocessing steps for the raw text data. \n",
    "For instance, we replace non-breaking space with space, convert uppercase letters to lowercase ones, and \n",
    "insert space between words and punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "overhead-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go .\tva !\n",
      "hi .\tsalut !\n",
      "run !\tcours !\n",
      "run !\tcourez !\n",
      "who ?\tqui ?\n",
      "wow !\tça alors !\n"
     ]
    }
   ],
   "source": [
    "def preprocess_nmt(text):\n",
    "    \"\"\"Preprocess the English-French dataset.\"\"\"\n",
    "    def no_space(char, prev_char):\n",
    "        return char in set(',.!?') and prev_char != ' '\n",
    "\n",
    "    # Replace non-breaking space with space, and convert uppercase letters to\n",
    "    # lowercase ones\n",
    "    text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
    "    # Insert space between words and punctuation marks\n",
    "    out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char\n",
    "           for i, char in enumerate(text)]\n",
    "    return ''.join(out)\n",
    "\n",
    "text = preprocess_nmt(raw_text)\n",
    "print(text[:80])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-techno",
   "metadata": {},
   "source": [
    "Different from character-level tokenization in Section 8.3, for machine translation we prefer word-level tokenization here (state-of-the-art models may use more advanced tokenization techniques). The following tokenize_nmt function tokenizes the the first num_examples text sequence pairs, where each token is either a word or a punctuation mark. This function returns two lists of token lists: source and target. Specifically, source[i] is a list of tokens from the  ith  text sequence in the source language (English here) and target[i] is that in the target language (French here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "exact-brooks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stop', '!'] ['stop', '!']\n",
      "['i', 'try', '.'] [\"j'essaye\", '.']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_nmt(text, num_examples=None):\n",
    "    \"\"\"Tokenize the English-French dataset.\"\"\"\n",
    "    source, target = [], []\n",
    "    for i, line in enumerate(text.split('\\n')):\n",
    "        if num_examples and i > num_examples:\n",
    "            break\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            source.append(parts[0].split(' '))\n",
    "            target.append(parts[1].split(' '))\n",
    "    return source, target\n",
    "\n",
    "source, target = tokenize_nmt(text)\n",
    "print(source[10], target[10])\n",
    "print(source[20], target[20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-defeat",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "\n",
    "Since the machine translation dataset consists of pairs of languages, we can build two vocabularies for \n",
    "both the source language and the target language separately. \n",
    "With word-level tokenization, the vocabulary size will be significantly larger than that using character-level \n",
    "tokenization. To alleviate this, here we treat infrequent tokens that appear less than 2 times as the \n",
    "same unknown (\\<unk\\>) token. Besides that, we specify additional special tokens s\n",
    "uch as for padding (\\<pad\\>) sequences to the same length in minibatches, and for marking the \n",
    "beginning (\\<bos\\>) or end (\\<eos\\>) of sequences. \n",
    "Such special tokens are commonly used in natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "focused-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = [] \n",
    "        # Sort according to frequencies\n",
    "        counter = count_corpus(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                  reverse=True)\n",
    "        # The index for the unknown token is 0\n",
    "        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens\n",
    "        uniq_tokens += [token for token, freq in self.token_freqs\n",
    "                        if freq >= min_freq and token not in uniq_tokens]\n",
    "        self.idx_to_token, self.token_to_idx = [], dict()\n",
    "        for token in uniq_tokens:\n",
    "            self.idx_to_token.append(token)\n",
    "            self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "\n",
    "def count_corpus(tokens):\n",
    "    \"\"\"Count token frequencies.\"\"\"\n",
    "    # Here `tokens` is a 1D list or 2D list\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # Flatten a list of token lists into a list of tokens\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "obvious-executive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10012"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_vocab = Vocab(source, min_freq=2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "len(src_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-thanksgiving",
   "metadata": {},
   "source": [
    "In machine translation, each example is a pair of source and target text sequences, where each text sequence may have different lengths.\n",
    "\n",
    "For computational efficiency, we can still process a minibatch of text sequences at one time by truncation and padding. Suppose that every sequence in the same minibatch should have the same length num_steps. If a text sequence has fewer than num_steps tokens, we will keep appending the special \\<pad\\> token to its end until its length reaches num_steps. Otherwise, we will truncate the text sequence by only taking its first num_steps tokens and discarding the remaining. In this way, every text sequence will have the same length to be loaded in minibatches of the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "expensive-creature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[47, 4, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def truncate_pad(line, num_steps, padding_token):\n",
    "    \"\"\"Truncate or pad sequences.\"\"\"\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps]  # Truncate\n",
    "    return line + [padding_token] * (num_steps - len(line))  # Pad\n",
    "\n",
    "\n",
    "truncate_pad(src_vocab[source[0]], 10, src_vocab['<pad>']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-airport",
   "metadata": {},
   "source": [
    "Now we define a function to transform text sequences into minibatches for training. We append the special \\<eos\\> token to the end of every sequence to indicate the end of the sequence. When a model is predicting by generating a sequence token after token, the generation of the \\<eos>\\ token can suggest that the output sequence is complete. Besides, we also record the length of each text sequence excluding the padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "accompanied-sandwich",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of 2 sentences:\n",
      "X: tensor([[ 6,  0,  4,  3,  1,  1,  1,  1],\n",
      "        [36,  5,  3,  1,  1,  1,  1,  1]], dtype=torch.int32)\n",
      "valid lengths for X: tensor([4, 3])\n",
      "Y: tensor([[10,  0,  4,  3,  1,  1,  1,  1],\n",
      "        [15,  0,  5,  3,  1,  1,  1,  1]], dtype=torch.int32)\n",
      "valid lengths for Y: tensor([4, 4])\n"
     ]
    }
   ],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"Construct a PyTorch data iterator.\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "def build_array_nmt(lines, vocab, num_steps):\n",
    "    \"\"\"Transform text sequences of machine translation into minibatches.\"\"\"\n",
    "    lines = [vocab[l] for l in lines]\n",
    "    lines = [l + [vocab['<eos>']] for l in lines]\n",
    "    array = torch.tensor(\n",
    "        [truncate_pad(l, num_steps, vocab['<pad>']) for l in lines])\n",
    "    valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)\n",
    "    return array, valid_len\n",
    "\n",
    "def load_data_nmt(data_path,batch_size, num_steps, num_examples=600):\n",
    "    \"\"\"Return the iterator and the vocabularies of the translation dataset.\"\"\"\n",
    "    text = preprocess_nmt(read_data_nmt(data_path))\n",
    "    source, target = tokenize_nmt(text, num_examples)\n",
    "    src_vocab = Vocab(source, min_freq=2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    tgt_vocab = Vocab(target, min_freq=2, reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    src_array, src_valid_len = build_array_nmt(source, src_vocab, num_steps)\n",
    "    tgt_array, tgt_valid_len = build_array_nmt(target, tgt_vocab, num_steps)\n",
    "    data_arrays = (src_array, src_valid_len, tgt_array, tgt_valid_len)\n",
    "    data_iter = load_array(data_arrays, batch_size)\n",
    "    return data_iter, src_vocab, tgt_vocab\n",
    "\n",
    "data_path = '../data/fra-eng/fra.txt'\n",
    "train_iter, src_vocab, tgt_vocab = load_data_nmt(data_path,batch_size=2, num_steps=8)\n",
    "for X, X_valid_len, Y, Y_valid_len in train_iter:\n",
    "    print('Batch of 2 sentences:')\n",
    "    print('X:', X.type(torch.int32))\n",
    "    print('valid lengths for X:', X_valid_len)\n",
    "    print('Y:', Y.type(torch.int32))\n",
    "    print('valid lengths for Y:', Y_valid_len)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "basic-native",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<unk>': 0, '<pad>': 1, '<bos>': 2, '<eos>': 3, '.': 4, '!': 5, 'i': 6, \"i'm\": 7, 'it': 8, 'go': 9, 'tom': 10, '?': 11, 'me': 12, 'get': 13, 'be': 14, 'up': 15, 'come': 16, 'we': 17, 'am': 18, 'this': 19, 'lost': 20, 'on': 21, 'won': 22, 'us': 23, \"it's\": 24, 'down': 25, 'no': 26, 'nice': 27, 'away': 28, 'you': 29, 'back': 30, 'try': 31, 'way': 32, 'fair': 33, 'out': 34, 'lazy': 35, 'help': 36, 'hold': 37, 'off': 38, 'grab': 39, 'how': 40, 'who': 41, 'got': 42, 'calm': 43, 'call': 44, 'he': 45, 'a': 46, 'good': 47, 'job': 48, 'did': 49, 'use': 50, 'over': 51, \"don't\": 52, 'forget': 53, 'run': 54, 'in': 55, 'home': 56, 'fun': 57, \"he's\": 58, 'sure': 59, 'here': 60, 'stop': 61, 'cool': 62, 'drive': 63, 'fat': 64, 'shut': 65, 'wake': 66, 'leave': 67, 'sit': 68, 'can': 69, 'fire': 70, 'cheers': 71, 'now': 72, 'left': 73, 'ok': 74, 'ask': 75, 'drop': 76, 'hang': 77, \"i'll\": 78, 'keep': 79, 'tell': 80, 'him': 81, 'ahead': 82, 'hurry': 83, 'fine': 84, 'died': 85, 'taste': 86, 'they': 87, 'watch': 88, 'what': 89, 'feel': 90, 'that': 91, 'beg': 92, 'hug': 93, 'fell': 94, 'really': 95, 'quit': 96, 'tried': 97, 'wet': 98, 'kiss': 99, 'still': 100, 'busy': 101, 'free': 102, 'late': 103, 'okay': 104, 'may': 105, 'she': 106, 'came': 107, 'terrific': 108, 'catch': 109, 'win': 110, 'follow': 111, 'cringed': 112, 'hi': 113, 'wait': 114, 'hello': 115, 'see': 116, 'attack': 117, 'hop': 118, 'know': 119, 'paid': 120, 'slow': 121, 'runs': 122, 'agree': 123, 'dozed': 124, 'stood': 125, 'swore': 126, 'hit': 127, 'ill': 128, 'sad': 129, 'join': 130, ',': 131, 'too': 132, 'open': 133, 'show': 134, 'take': 135, 'wash': 136, 'them': 137, 'man': 138, 'beats': 139, 'find': 140, 'fix': 141, 'have': 142, 'phoned': 143, 'refuse': 144, 'rested': 145, 'saw': 146, 'stayed': 147, 'cold': 148, 'deaf': 149, 'full': 150, 'game': 151, 'rich': 152, 'sick': 153, 'tidy': 154, 'ugly': 155, 'weak': 156, 'well': 157, \"i've\": 158, 'works': 159, 'his': 160, 'new': 161, \"let's\": 162, 'look': 163, 'marry': 164, 'save': 165, 'speak': 166, 'trust': 167, 'some': 168, 'warn': 169, 'for': 170, 'write': 171, 'seated': 172, 'soon': 173, 'dogs': 174, 'bark': 175, 'die': 176, 'excuse': 177, 'ready': 178, 'to': 179, 'bed': 180, 'luck': 181, 'is': 182, \"how's\": 183}\n"
     ]
    }
   ],
   "source": [
    "print(src_vocab.token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "earned-frederick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<unk>': 0, '<pad>': 1, '<bos>': 2, '<eos>': 3, '.': 4, '!': 5, 'je': 6, 'suis': 7, 'tom': 8, '?': 9, \"j'ai\": 10, 'nous': 11, 'ça': 12, \"c'est\": 13, 'est': 14, 'à': 15, 'va': 16, 'bien': 17, 'il': 18, 'en': 19, 'soyez': 20, 'j’ai': 21, 'pas': 22, 'un': 23, 'qui': 24, 'gagné': 25, 'sois': 26, 'me': 27, 'tomber': 28, 'la': 29, 'ne': 30, 'ceci': 31, 'de': 32, 'vais': 33, 'bon': 34, 'venez': 35, 'le': 36, 'chez': 37, \"j'en\": 38, 'avons': 39, 'calme': 40, 'viens': 41, 'vous': 42, 'a': 43, 'moi': 44, 'au': 45, \"l'ai\": 46, 'emporté': 47, 'perdu': 48, 'allez': 49, 'plus': 50, 'fait': 51, 'comme': 52, 'ici': 53, 'feu': 54, 'maintenant': 55, 'compris': 56, 'sais': 57, 'gentil': 58, 'dégage': 59, 'malade': 60, 'fûmes': 61, 'été': 62, 'elle': 63, 'assieds-toi': 64, 'salut': 65, 'cours': 66, 'vas-y': 67, 'question': 68, 'juste': 69, 'entrez': 70, 'laisse': 71, 'chercher': 72, 'pars': 73, 'maison': 74, 'tiens': 75, 'tenez': 76, 'fais': 77, 'réveille-toi': 78, 'suis-je': 79, 'trouve': 80, 'trouvez': 81, 'boulot': 82, 'les': 83, \"m'en\": 84, 'paresseux': 85, 'certain': 86, 'puis-je': 87, 'aller': 88, 'asseyez-vous': 89, 'pouvons-nous': 90, 'attrape': 91, 'attrapez': 92, 'courez': 93, 'attends': 94, 'attendez': 95, 'poursuis': 96, 'continuez': 97, 'santé': 98, 'merci': 99, ',': 100, 'pigé': 101, 'capté': 102, 'dans': 103, 'tes': 104, 'bras': 105, 'tombé': 106, 'parti': 107, 'partie': 108, 'payé': 109, 'hors': 110, 'aucune': 111, 'essaye': 112, 'demande': 113, 'fantastique': 114, 'calmes': 115, 'détendu': 116, 'équitable': 117, 'gentille': 118, 'entre': 119, 'laissez': 120, 'sortez': 121, 'sors': 122, 'te': 123, 'faire': 124, 'foutre': 125, 'rentrez': 126, 'rentre': 127, 'doucement': 128, 'peu': 129, 'court': 130, 'aide-moi': 131, 'du': 132, 'debout': 133, 'signe': 134, 'gras': 135, 'gros': 136, 'triste': 137, 'mouillé': 138, 'joignez-vous': 139, 'ferme-la': 140, 'tard': 141, 'réveillez-vous': 142, 'battus': 143, 'battues': 144, 'défaits': 145, 'défaites': 146, 'tu': 147, 'recule\\u2009': 148, 'reculez': 149, 'homme': 150, 'appelle': 151, 'rouler': 152, 'lâche-toi': 153, 'aide': 154, 'fais-moi': 155, 'refuse': 156, 'vu': 157, 'occupé': 158, 'froid': 159, 'ai': 160, 'libre': 161, 'retard': 162, 'fainéant': 163, 'paresseuse': 164, 'fainéante': 165, 'porte': 166, 'riche': 167, 'sûr': 168, 'faible': 169, 'bizarre': 170, 'allons-y': 171, 'partir': 172, 'y': 173, 'fort': 174, 'ils': 175, 'gagnèrent': 176, 'elles': 177, 'ont': 178, 'venu': 179, 'mort': 180, 'confiance': 181, 'quoi': 182, \"qu'est-ce\": 183, \"qu'on\": 184, \"s'est\": 185, 'calmez-vous': 186, 'bientôt': 187, 'chiens': 188, 'aboient': 189, 'touche': 190, 'oublie': 191, 'oublie-le': 192, 'emploi': 193, 'lit': 194, 'bonne': 195, 'chance': 196, 'comment': 197, 'prie': 198, 'mouvement': 199, 'recul': 200}\n"
     ]
    }
   ],
   "source": [
    "print(tgt_vocab.token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-fiction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
