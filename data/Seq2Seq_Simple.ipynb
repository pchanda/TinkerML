{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "satellite-murray",
   "metadata": {},
   "source": [
    "# Simple Seq2Seq machine translation using GRU based encoder-decoder architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "medieval-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-beijing",
   "metadata": {},
   "source": [
    "## Preparing the dataset for Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "looking-relation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVa !\n",
      "Hi.\tSalut !\n",
      "go .\tva !\n",
      "hi .\tsalut !\n",
      "run !\tcours !\n",
      "run !\tcourez !\n",
      "who ?\tqui ?\n",
      "wow !\tça alors !\n",
      "['stop', '!'] ['stop', '!']\n",
      "['i', 'try', '.'] [\"j'essaye\", '.']\n",
      "Batch of 2 sentences:\n",
      "X: tensor([[  6, 124,   4,   3,   1,   1,   1,   1],\n",
      "        [  6,  18, 101,   4,   3,   1,   1,   1]], dtype=torch.int32)\n",
      "valid lengths for X: tensor([4, 5])\n",
      "Y: tensor([[  6,  27,   7,   0,   4,   3,   1,   1],\n",
      "        [  6,   7, 158,   4,   3,   1,   1,   1]], dtype=torch.int32)\n",
      "valid lengths for Y: tensor([6, 5])\n",
      "{'<unk>': 0, '<pad>': 1, '<bos>': 2, '<eos>': 3, '.': 4, '!': 5, 'i': 6, \"i'm\": 7, 'it': 8, 'go': 9, 'tom': 10, '?': 11, 'me': 12, 'get': 13, 'be': 14, 'up': 15, 'come': 16, 'we': 17, 'am': 18, 'this': 19, 'lost': 20, 'on': 21, 'won': 22, 'us': 23, \"it's\": 24, 'down': 25, 'no': 26, 'nice': 27, 'away': 28, 'you': 29, 'back': 30, 'try': 31, 'way': 32, 'fair': 33, 'out': 34, 'lazy': 35, 'help': 36, 'hold': 37, 'off': 38, 'grab': 39, 'how': 40, 'who': 41, 'got': 42, 'calm': 43, 'call': 44, 'he': 45, 'a': 46, 'good': 47, 'job': 48, 'did': 49, 'use': 50, 'over': 51, \"don't\": 52, 'forget': 53, 'run': 54, 'in': 55, 'home': 56, 'fun': 57, \"he's\": 58, 'sure': 59, 'here': 60, 'stop': 61, 'cool': 62, 'drive': 63, 'fat': 64, 'shut': 65, 'wake': 66, 'leave': 67, 'sit': 68, 'can': 69, 'fire': 70, 'cheers': 71, 'now': 72, 'left': 73, 'ok': 74, 'ask': 75, 'drop': 76, 'hang': 77, \"i'll\": 78, 'keep': 79, 'tell': 80, 'him': 81, 'ahead': 82, 'hurry': 83, 'fine': 84, 'died': 85, 'taste': 86, 'they': 87, 'watch': 88, 'what': 89, 'feel': 90, 'that': 91, 'beg': 92, 'hug': 93, 'fell': 94, 'really': 95, 'quit': 96, 'tried': 97, 'wet': 98, 'kiss': 99, 'still': 100, 'busy': 101, 'free': 102, 'late': 103, 'okay': 104, 'may': 105, 'she': 106, 'came': 107, 'terrific': 108, 'catch': 109, 'win': 110, 'follow': 111, 'cringed': 112, 'hi': 113, 'wait': 114, 'hello': 115, 'see': 116, 'attack': 117, 'hop': 118, 'know': 119, 'paid': 120, 'slow': 121, 'runs': 122, 'agree': 123, 'dozed': 124, 'stood': 125, 'swore': 126, 'hit': 127, 'ill': 128, 'sad': 129, 'join': 130, ',': 131, 'too': 132, 'open': 133, 'show': 134, 'take': 135, 'wash': 136, 'them': 137, 'man': 138, 'beats': 139, 'find': 140, 'fix': 141, 'have': 142, 'phoned': 143, 'refuse': 144, 'rested': 145, 'saw': 146, 'stayed': 147, 'cold': 148, 'deaf': 149, 'full': 150, 'game': 151, 'rich': 152, 'sick': 153, 'tidy': 154, 'ugly': 155, 'weak': 156, 'well': 157, \"i've\": 158, 'works': 159, 'his': 160, 'new': 161, \"let's\": 162, 'look': 163, 'marry': 164, 'save': 165, 'speak': 166, 'trust': 167, 'some': 168, 'warn': 169, 'for': 170, 'write': 171, 'seated': 172, 'soon': 173, 'dogs': 174, 'bark': 175, 'die': 176, 'excuse': 177, 'ready': 178, 'to': 179, 'bed': 180, 'luck': 181, 'is': 182, \"how's\": 183}\n",
      "{'<unk>': 0, '<pad>': 1, '<bos>': 2, '<eos>': 3, '.': 4, '!': 5, 'je': 6, 'suis': 7, 'tom': 8, '?': 9, \"j'ai\": 10, 'nous': 11, 'ça': 12, \"c'est\": 13, 'est': 14, 'à': 15, 'va': 16, 'bien': 17, 'il': 18, 'en': 19, 'soyez': 20, 'j’ai': 21, 'pas': 22, 'un': 23, 'qui': 24, 'gagné': 25, 'sois': 26, 'me': 27, 'tomber': 28, 'la': 29, 'ne': 30, 'ceci': 31, 'de': 32, 'vais': 33, 'bon': 34, 'venez': 35, 'le': 36, 'chez': 37, \"j'en\": 38, 'avons': 39, 'calme': 40, 'viens': 41, 'vous': 42, 'a': 43, 'moi': 44, 'au': 45, \"l'ai\": 46, 'emporté': 47, 'perdu': 48, 'allez': 49, 'plus': 50, 'fait': 51, 'comme': 52, 'ici': 53, 'feu': 54, 'maintenant': 55, 'compris': 56, 'sais': 57, 'gentil': 58, 'dégage': 59, 'malade': 60, 'fûmes': 61, 'été': 62, 'elle': 63, 'assieds-toi': 64, 'salut': 65, 'cours': 66, 'vas-y': 67, 'question': 68, 'juste': 69, 'entrez': 70, 'laisse': 71, 'chercher': 72, 'pars': 73, 'maison': 74, 'tiens': 75, 'tenez': 76, 'fais': 77, 'réveille-toi': 78, 'suis-je': 79, 'trouve': 80, 'trouvez': 81, 'boulot': 82, 'les': 83, \"m'en\": 84, 'paresseux': 85, 'certain': 86, 'puis-je': 87, 'aller': 88, 'asseyez-vous': 89, 'pouvons-nous': 90, 'attrape': 91, 'attrapez': 92, 'courez': 93, 'attends': 94, 'attendez': 95, 'poursuis': 96, 'continuez': 97, 'santé': 98, 'merci': 99, ',': 100, 'pigé': 101, 'capté': 102, 'dans': 103, 'tes': 104, 'bras': 105, 'tombé': 106, 'parti': 107, 'partie': 108, 'payé': 109, 'hors': 110, 'aucune': 111, 'essaye': 112, 'demande': 113, 'fantastique': 114, 'calmes': 115, 'détendu': 116, 'équitable': 117, 'gentille': 118, 'entre': 119, 'laissez': 120, 'sortez': 121, 'sors': 122, 'te': 123, 'faire': 124, 'foutre': 125, 'rentrez': 126, 'rentre': 127, 'doucement': 128, 'peu': 129, 'court': 130, 'aide-moi': 131, 'du': 132, 'debout': 133, 'signe': 134, 'gras': 135, 'gros': 136, 'triste': 137, 'mouillé': 138, 'joignez-vous': 139, 'ferme-la': 140, 'tard': 141, 'réveillez-vous': 142, 'battus': 143, 'battues': 144, 'défaits': 145, 'défaites': 146, 'tu': 147, 'recule\\u2009': 148, 'reculez': 149, 'homme': 150, 'appelle': 151, 'rouler': 152, 'lâche-toi': 153, 'aide': 154, 'fais-moi': 155, 'refuse': 156, 'vu': 157, 'occupé': 158, 'froid': 159, 'ai': 160, 'libre': 161, 'retard': 162, 'fainéant': 163, 'paresseuse': 164, 'fainéante': 165, 'porte': 166, 'riche': 167, 'sûr': 168, 'faible': 169, 'bizarre': 170, 'allons-y': 171, 'partir': 172, 'y': 173, 'fort': 174, 'ils': 175, 'gagnèrent': 176, 'elles': 177, 'ont': 178, 'venu': 179, 'mort': 180, 'confiance': 181, 'quoi': 182, \"qu'est-ce\": 183, \"qu'on\": 184, \"s'est\": 185, 'calmez-vous': 186, 'bientôt': 187, 'chiens': 188, 'aboient': 189, 'touche': 190, 'oublie': 191, 'oublie-le': 192, 'emploi': 193, 'lit': 194, 'bonne': 195, 'chance': 196, 'comment': 197, 'prie': 198, 'mouvement': 199, 'recul': 200}\n"
     ]
    }
   ],
   "source": [
    "from ProcessData import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-greek",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "actual-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(nn.Module):\n",
    "    \"\"\"The RNN encoder for sequence to sequence learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0):\n",
    "        \n",
    "        super(Seq2SeqEncoder, self).__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Input X: (`batch_size`, `num_steps`, `input_size`)\n",
    "        \n",
    "        X = self.embedding(X)\n",
    "        # After embedding X: (`batch_size`, `num_steps`, `embed_size`)\n",
    "        \n",
    "        # When batch_first is True:\n",
    "        # in RNN models, the first axis corresponds to batch_size \n",
    "        #                the second axis corresponds to num_steps\n",
    "        #                the first axis corresponds to embed_dim\n",
    "        \n",
    "        # When state is not mentioned, it defaults to zeros\n",
    "        output, state = self.rnn(X)\n",
    "        # `output` shape: (`batch_size`, `num_steps`, `num_hiddens`)\n",
    "        # `state` shape:  (`num_layers`, `batch_size`, `num_hiddens`)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "protecting-monroe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4, 16])\n",
      "torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=1)\n",
    "encoder.eval()\n",
    "X = torch.zeros((5, 4), dtype=torch.long)\n",
    "enc_output, enc_state = encoder(X)\n",
    "\n",
    "print(enc_output.shape)\n",
    "print(enc_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-affect",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "greater-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(nn.Module):\n",
    "    \"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0):\n",
    "        \n",
    "        super(Seq2SeqDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout,batch_first=True)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # Inputs: \n",
    "        # X : (`batch_size`, `num_steps`, `input_size`)\n",
    "        # initial hidden state : (`num_layers`, `batch_size`, `num_hiddens`) , \n",
    "        # This comes from hidden state output from encoder.\n",
    "                \n",
    "        X = self.embedding(X)\n",
    "        # After embedding X: (`batch_size`, `num_steps`, `embed_size`)\n",
    "        \n",
    "        # Context is last layer hidden state from last timestep of encoder \n",
    "        \n",
    "        # last layer hidden state from last time step of encoder\n",
    "        last_layer_state = state[-1] # shape (`batch_size`,`num_hiddens`)\n",
    "        \n",
    "        # context is last timestep hidden state of encoder.\n",
    "        # Broadcast `context` so it has the same `num_steps` as `X` \n",
    "        context = last_layer_state.repeat(X.shape[1], 1, 1).permute(1,0,2)\n",
    "        \n",
    "        # context has now shape (`batch_size`,`num_steps`,`num_hiddens`) \n",
    "        \n",
    "        # concat(X,context) = X_and_context of shape (`batch_size`,`num_steps`,`emb_dim + num_hiddens`)\n",
    "        X_and_context = torch.cat((X, context), 2)\n",
    "        \n",
    "        output, state = self.rnn(X_and_context, state)\n",
    "        # output : (`batch_size`,`num_steps`,`num_hiddens`)\n",
    "        # state : (`num_layers`,`batch_size`,`num_hiddens`), this is final timestep hidden state of decoder\n",
    "        \n",
    "        output = self.dense(output)\n",
    "        \n",
    "        # final output of decoder :\n",
    "        # `output` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
    "        # `state` shape: (`num_layers`, `batch_size`, `num_hiddens`)\n",
    "\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "historic-evidence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4, 10]), torch.Size([1, 5, 16]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=1)\n",
    "decoder.eval()\n",
    "\n",
    "output, state = decoder(X, enc_state)\n",
    "output.shape, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "quarterly-season",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 1, 10]), torch.Size([1, 5, 16]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can feed input consisting of a single timestep as well.\n",
    "dec_X = torch.from_numpy(np.zeros((5,1))).long()\n",
    "print(dec_X.shape) # batch_size=5, num_steps=1\n",
    "\n",
    "output, state = decoder(dec_X, enc_state)\n",
    "output.shape, state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-booth",
   "metadata": {},
   "source": [
    "## Putting encoder and decoder together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "duplicate-designer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Seq2SeqEncoder(\n",
       "    (embedding): Embedding(10, 8)\n",
       "    (rnn): GRU(8, 16, batch_first=True)\n",
       "  )\n",
       "  (decoder): Seq2SeqDecoder(\n",
       "    (embedding): Embedding(10, 8)\n",
       "    (rnn): GRU(24, 16, batch_first=True)\n",
       "    (dense): Linear(in_features=16, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"The base class for the encoder-decoder architecture.\"\"\"\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X):\n",
    "        enc_output, enc_state = self.encoder(enc_X)\n",
    "        return self.decoder(dec_X, enc_state)    \n",
    "    \n",
    "encoder_decoder = EncoderDecoder(encoder,decoder)\n",
    "encoder_decoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-angle",
   "metadata": {},
   "source": [
    "## Allow parts of sequence to be masked as we have variable length sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "capable-finger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has 2 sequences :\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "Assume that first sequence has 1 valid elements, second sequence has 2 valid elements tensor([1, 2])\n",
      "After masking:\n",
      " tensor([[1, 0, 0],\n",
      "        [4, 5, 0]])\n"
     ]
    }
   ],
   "source": [
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"Mask irrelevant entries in sequences.\"\"\"\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen),device=X.device)[None, :] < valid_len[:, None]\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "X = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "valid_lens = torch.tensor([1, 2])\n",
    "print('Input has 2 sequences :\\n',X)\n",
    "print('Assume that first sequence has 1 valid elements, second sequence has 2 valid elements', valid_lens)\n",
    "\n",
    "print('After masking:\\n',sequence_mask(X, valid_lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-cruise",
   "metadata": {},
   "source": [
    "## Build cross entropy loss using masked sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "commercial-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"The softmax cross-entropy loss with masks.\"\"\"\n",
    "    # `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n",
    "    # `label` shape: (`batch_size`, `num_steps`)\n",
    "    # `valid_len` shape: (`batch_size`,)\n",
    "    \n",
    "    def forward(self, pred, label, valid_len):\n",
    "        \n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len).float()\n",
    "        self.reduction='none'\n",
    "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(pred.permute(0, 2, 1), label)\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weighted_loss\n",
    "\n",
    "loss = MaskedSoftmaxCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-intro",
   "metadata": {},
   "source": [
    "## Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "serial-debut",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device= cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Seq2SeqEncoder(\n",
       "    (embedding): Embedding(184, 32)\n",
       "    (rnn): GRU(32, 32, num_layers=2, batch_first=True, dropout=0.1)\n",
       "  )\n",
       "  (decoder): Seq2SeqDecoder(\n",
       "    (embedding): Embedding(201, 32)\n",
       "    (rnn): GRU(64, 32, num_layers=2, batch_first=True, dropout=0.1)\n",
       "    (dense): Linear(in_features=32, out_features=201, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_size = 32\n",
    "num_hiddens = 32\n",
    "num_layers = 2\n",
    "\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "num_steps = 10\n",
    "lr = 0.005\n",
    "num_epochs = 300 \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device=',device)\n",
    "\n",
    "data_path = '../data/fra-eng/fra.txt'\n",
    "train_iter, src_vocab, tgt_vocab = load_data_nmt(data_path, batch_size, num_steps)\n",
    "\n",
    "encoder = Seq2SeqEncoder( len(src_vocab), embed_size, num_hiddens, num_layers, dropout )\n",
    "decoder = Seq2SeqDecoder( len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout )\n",
    "\n",
    "net = EncoderDecoder(encoder, decoder)\n",
    "\n",
    "net.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-queen",
   "metadata": {},
   "source": [
    "## Initialize weights in GRU layers of encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "based-intermediate",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Seq2SeqEncoder(\n",
       "    (embedding): Embedding(184, 32)\n",
       "    (rnn): GRU(32, 32, num_layers=2, batch_first=True, dropout=0.1)\n",
       "  )\n",
       "  (decoder): Seq2SeqDecoder(\n",
       "    (embedding): Embedding(201, 32)\n",
       "    (rnn): GRU(64, 32, num_layers=2, batch_first=True, dropout=0.1)\n",
       "    (dense): Linear(in_features=32, out_features=201, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def xavier_init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "    if type(m) == nn.GRU:\n",
    "        #initialize biases and weights\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant(param, 0.0)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.xavier_uniform_(m._parameters[name])\n",
    "\n",
    "net.apply(xavier_init_weights)\n",
    "net.to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = MaskedSoftmaxCELoss()\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-spanish",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "powerful-mayor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.2090059375725408\n",
      "20 0.1508961954908735\n",
      "30 0.1133520789898929\n",
      "40 0.08690908657946206\n",
      "50 0.07063316426282791\n",
      "60 0.060021722659216487\n",
      "70 0.05148320665325719\n",
      "80 0.044760870867852666\n",
      "90 0.039765634578027766\n",
      "100 0.0361578657795059\n",
      "110 0.0323923955232034\n",
      "120 0.030629563359752598\n",
      "130 0.02831386459572772\n",
      "140 0.027698246806871697\n",
      "150 0.02605452747712779\n",
      "160 0.025122531799292386\n",
      "170 0.02461659434460036\n",
      "180 0.023439871518924356\n",
      "190 0.022659589061114038\n",
      "200 0.022360768012294897\n",
      "210 0.021564902962778836\n",
      "220 0.021263796998871824\n",
      "230 0.021205942258268114\n",
      "240 0.02060909357562585\n",
      "250 0.020626334025605556\n",
      "260 0.020763019181198605\n",
      "270 0.020957735334499197\n",
      "280 0.020493817395086787\n",
      "290 0.020140862192801725\n",
      "300 0.02038786731241069\n",
      "loss 0.020, 29684.8 tokens/sec on cuda:0\n"
     ]
    }
   ],
   "source": [
    "class Accumulator:\n",
    "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "        \n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    \n",
    "    \n",
    "def grad_clipping(net, theta):\n",
    "    \"\"\"Clip the gradient.\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm\n",
    "\n",
    "            \n",
    "cum_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    metric = Accumulator(2)  # Sum of training loss, no. of tokens\n",
    "    \n",
    "    for batch in train_iter:\n",
    "        X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "        bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0], device=device).reshape(-1, 1)\n",
    "        dec_input = torch.cat([bos, Y[:, :-1]], 1)  # Teacher forcing\n",
    "        Y_hat, _ = net(X, dec_input)\n",
    "        l = loss(Y_hat, Y, Y_valid_len)\n",
    "        l.sum().backward()  # Make the loss scalar for `backward`\n",
    "        grad_clipping(net, 1)\n",
    "        num_tokens = Y_valid_len.sum()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            metric.add(l.sum(), num_tokens)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(epoch+1,metric[0] / metric[1])\n",
    "        cum_losses.append(metric[0] / metric[1])\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    \n",
    "print(f'loss {metric[0] / metric[1]:.3f}, {metric[1] / elapsed_time:.1f} 'f'tokens/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-placement",
   "metadata": {},
   "source": [
    "## Plot the loss over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acquired-watson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = range(len(cum_losses))\n",
    "plt.plot(X, cum_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-diploma",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "actual-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_a_sentence(src_sentence, src_vocab, bos_token, num_steps):\n",
    "\n",
    "    # First process the src_sentence, tokenize and truncate/pad it.\n",
    "    #src_sentence : a sentence to translate\n",
    "\n",
    "    #Tokenize the sentence\n",
    "    src_sentence_words = src_sentence.lower().split(' ')\n",
    "    print('src sentence words = ',src_sentence_words)\n",
    "\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')]\n",
    "    src_tokens = src_tokens + [src_vocab['<eos>']]\n",
    "    print('src_tokens = ',src_tokens)\n",
    "\n",
    "    enc_valid_len = torch.tensor([len(src_tokens)], device=device)\n",
    "\n",
    "    #Truncate the sentence to num_steps if the sentence is longer. If shorter, pad the sentence.\n",
    "    print('Truncating/padding to length',num_steps)\n",
    "    padding_token = src_vocab['<pad>']\n",
    "    if len(src_tokens) > num_steps:\n",
    "        line[:num_steps]  # Truncate\n",
    "    #Pad \n",
    "    src_tokens = src_tokens + [padding_token] * (num_steps - len(src_tokens))\n",
    "    print('After truncating/padding',src_tokens,'\\n')\n",
    "    \n",
    "    # Next convert the src_tokens to a tensor to be fed to the decoder one word at a timestep\n",
    "    # Covert src_tokens to a tesnor, add the batch axis\n",
    "    enc_X = torch.unsqueeze(torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0)\n",
    "    # Now shape of enc_X : (`batch_size` , `num_steps`) = (1,10)\n",
    "    # Pass it through the encoder\n",
    "    enc_output, enc_state = net.encoder(enc_X)\n",
    "\n",
    "    # feed the decoder one word token at a time\n",
    "    # prepare the first token for decoder : beginning of sentence\n",
    "    dec_X = torch.unsqueeze(torch.tensor([tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0)\n",
    "    \n",
    "    #Initialize input state for the decoder to be the final timestep state of the encoder\n",
    "    dec_input_state = enc_state\n",
    "    output_token_seq = []\n",
    "    for _ in range(num_steps):\n",
    "        curr_output, curr_dec_state = decoder(dec_X, dec_input_state)\n",
    "        dec_input_state = curr_dec_state\n",
    "        \n",
    "        # curr_output is of shape (`batch_size`, `num_steps`, `len(tgt_vocab)`) = (1,10,201)\n",
    "        # Use the token with the highest prediction likelihood as the input of the decoder for the next time step\n",
    "        dec_X = curr_output.argmax(dim=2) #next timestep input for decoder\n",
    "        \n",
    "        #remove batch_size dimension as we are working with single sentences\n",
    "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        #eos predicted, stop\n",
    "        if pred == tgt_vocab['<eos>']:\n",
    "            break\n",
    "        output_token_seq.append(pred)\n",
    "    \n",
    "    return output_token_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-welding",
   "metadata": {},
   "source": [
    "### Lets look for some translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "intelligent-jewel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src sentence words =  ['go', '.']\n",
      "src_tokens =  [9, 4, 3]\n",
      "Truncating/padding to length 10\n",
      "After truncating/padding [9, 4, 3, 1, 1, 1, 1, 1, 1, 1] \n",
      "\n",
      "Actual    translation: english:go . => french:va !\n",
      "Predicted translation: english:go . => french:va au gagné ?\n",
      "-------------------------------------------------------------------\n",
      "src sentence words =  ['i', 'lost', '.']\n",
      "src_tokens =  [6, 20, 4, 3]\n",
      "Truncating/padding to length 10\n",
      "After truncating/padding [6, 20, 4, 3, 1, 1, 1, 1, 1, 1] \n",
      "\n",
      "Actual    translation: english:i lost . => french:j'ai perdu .\n",
      "Predicted translation: english:i lost . => french:j'ai perdu .\n",
      "-------------------------------------------------------------------\n",
      "src sentence words =  [\"he's\", 'calm', '.']\n",
      "src_tokens =  [58, 43, 4, 3]\n",
      "Truncating/padding to length 10\n",
      "After truncating/padding [58, 43, 4, 3, 1, 1, 1, 1, 1, 1] \n",
      "\n",
      "Actual    translation: english:he's calm . => french:il est calme .\n",
      "Predicted translation: english:he's calm . => french:il est mouillé !\n",
      "-------------------------------------------------------------------\n",
      "src sentence words =  [\"i'm\", 'home', '.']\n",
      "src_tokens =  [7, 56, 4, 3]\n",
      "Truncating/padding to length 10\n",
      "After truncating/padding [7, 56, 4, 3, 1, 1, 1, 1, 1, 1] \n",
      "\n",
      "Actual    translation: english:i'm home . => french:je suis chez moi .\n",
      "Predicted translation: english:i'm home . => french:je suis chez moi <unk> .\n",
      "-------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "english_batch = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "french_batch = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "\n",
    "bos_token = tgt_vocab['<bos>']\n",
    "for eng_sent, fr_sent in zip(english_batch,french_batch):\n",
    "    \n",
    "    fr_sent_predicted_tokens = translate_a_sentence(eng_sent, src_vocab, bos_token, num_steps)\n",
    "    fr_sent_predicted = ' '.join(tgt_vocab.to_tokens(fr_sent_predicted_tokens))\n",
    "    print(f'Actual    translation: english:{eng_sent} => french:{fr_sent}')\n",
    "    print(f'Predicted translation: english:{eng_sent} => french:{fr_sent_predicted}')\n",
    "    print('-------------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
