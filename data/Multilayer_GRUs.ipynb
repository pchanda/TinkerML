{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import time\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A : tensor([[1., 0., 0., 0., 0., 0.]])\n",
      "B : tensor([[0., 1., 0., 0., 0., 0.]])\n",
      "C : tensor([[0., 0., 1., 0., 0., 0.]])\n",
      "D : tensor([[0., 0., 0., 1., 0., 0.]])\n",
      "E : tensor([[0., 0., 0., 0., 1., 0.]])\n",
      "# : tensor([[0., 0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "pad_char = '#'\n",
    "all_chars = 'ABCDE'+pad_char\n",
    "n_chars = len(all_chars)\n",
    "\n",
    "# Find char index from all_chars, e.g. \"a\" = 0\n",
    "def charToIndex(char):\n",
    "    return all_chars.find(char)\n",
    "\n",
    "# Just for demonstration, turn a char into a <1 x n_chars> Tensor\n",
    "def charToTensor_one_hot(char):\n",
    "    tensor = torch.zeros(1, n_chars)\n",
    "    tensor[0][charToIndex(char)] = 1\n",
    "    return tensor\n",
    "\n",
    "def charToTensor(char):\n",
    "    tensor = torch.zeros(1,dtype=torch.long)\n",
    "    tensor[0] = charToIndex(char)\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_chars>,\n",
    "# or an array of one-hot char vectors\n",
    "def seqToTensor_one_hot(seq):\n",
    "    tensor = torch.zeros(len(seq),1, n_chars)\n",
    "    for idx, char in enumerate(seq):\n",
    "        tensor[idx][0][charToIndex(char)] = 1\n",
    "    return tensor\n",
    "\n",
    "def seqToTensor(seq):\n",
    "    tensor = torch.zeros(len(seq), dtype=torch.long)\n",
    "    for idx, char in enumerate(seq):\n",
    "        tensor[idx] = int(charToIndex(char))\n",
    "    return tensor\n",
    "\n",
    "\n",
    "for ch in all_chars:\n",
    "    print(ch,':',charToTensor_one_hot(ch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare some tensor data for input : 2 character sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor shape= torch.Size([2, 10, 6])\n",
      "input tensor=\n",
      " tensor([[[0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 1.],\n",
      "         [1., 0., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 1.],\n",
      "         [1., 0., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "sequences = ['AABC','AAAACC']\n",
    "\n",
    "batch_size = len(sequences)\n",
    "\n",
    "max_seqlen = 10\n",
    "\n",
    "seq_tensors = []\n",
    "for seq in sequences:\n",
    "    seq_tensor = seqToTensor_one_hot(seq)\n",
    "    seq_tensors.append(torch.squeeze(seq_tensor))\n",
    "    \n",
    "pad_char_tensor = charToTensor_one_hot(pad_char) #tensor corresponding to pad_char\n",
    "            \n",
    "batch_tensor = pad_char_tensor.repeat(batch_size, max_seqlen,1)\n",
    "\n",
    "#print('batch_names_tensor',batch_names_tensor.shape)\n",
    "\n",
    "for i,t in enumerate(seq_tensors):\n",
    "    num_chars = t.shape[0]\n",
    "    batch_tensor[i,-num_chars:,:] = t #Left padding is done with pad_char\n",
    "    \n",
    "print('input tensor shape=',batch_tensor.shape)\n",
    "print('input tensor=\\n',batch_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Simple GRU Layer (3 layers)\n",
    "### Takes input an entire sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGRU(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, num_layers=1, bidirectional=False, hidden_dim=10, printVars=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        print()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gru = nn.GRU(input_size, hidden_dim, num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "        \n",
    "        self.num_layers = num_layers        \n",
    "        self.num_directions = 2 if bidirectional==True else 1        \n",
    "        \n",
    "        #dim=2 as we are doing softmax across the last dimension of output_size\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "        self.hidden = None\n",
    "        self.printVars = printVars #run the print statements in forward ?\n",
    "        \n",
    "        #initialize biases and weights to some fixed value for testing\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant(param, 0.3)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.constant(param, 0.3) #nn.init.xavier_normal(param)\n",
    "                \n",
    "                \n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        hidden_dim = self.hidden_dim\n",
    "        weight = next(self.parameters()).data\n",
    "        h_0 = weight.new(self.num_directions*self.num_layers, batch_size, hidden_dim).zero_()\n",
    "        return h_0\n",
    "\n",
    "\n",
    "    def forward(self, batch_of_words):\n",
    "        \n",
    "        batch_size = batch_of_words.shape[0]\n",
    "        \n",
    "        #This is stateless GRU, so hidden states are initialized for every forward pass.\n",
    "        #The hidden states are not preserved across batches.\n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        h_0 = self.hidden #initial hidden state, shape (num_direction*num_layers , batch_size, hidden_dim)\n",
    "        x = batch_of_words\n",
    "        \n",
    "        if self.printVars:\n",
    "            print('forward: h_0.shape',h_0.shape)\n",
    "            print('forward: input to gru, x =',x.shape)\n",
    "        \n",
    "        output, self.hidden = self.gru(x, self.hidden)\n",
    "        \n",
    "        #output: output features h_t from the last layer of the GRU for each timestep=t\n",
    "        #self.hidden : tensor containing the hidden state for the last timestep t = seq_len\n",
    "        \n",
    "        if self.printVars:\n",
    "            print('\\ngru_output=',output.shape,'\\n',output) #output from final layer for all timesteps.\n",
    "            print('\\nh_out=',self.hidden.shape,'\\n',self.hidden) #hidden state from last timestep for all layers\n",
    "        \n",
    "        \n",
    "        return output, self.hidden\n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size= 2\n",
      "input_size = 6\n",
      "n_hidden = 4\n",
      "n_layers =  3\n",
      "\n",
      "input to gru =  torch.Size([2, 10, 6])\n",
      "\n",
      " all_step_gru_output= torch.Size([2, 10, 4]) \n",
      " tensor([[[0.1908, 0.1908, 0.1908, 0.1908],\n",
      "         [0.3296, 0.3296, 0.3296, 0.3296],\n",
      "         [0.4295, 0.4295, 0.4295, 0.4295],\n",
      "         [0.5042, 0.5042, 0.5042, 0.5042],\n",
      "         [0.5623, 0.5623, 0.5623, 0.5623],\n",
      "         [0.6090, 0.6090, 0.6090, 0.6090],\n",
      "         [0.6475, 0.6475, 0.6475, 0.6475],\n",
      "         [0.6800, 0.6800, 0.6800, 0.6800],\n",
      "         [0.7077, 0.7077, 0.7077, 0.7077],\n",
      "         [0.7317, 0.7317, 0.7317, 0.7317]],\n",
      "\n",
      "        [[0.1908, 0.1908, 0.1908, 0.1908],\n",
      "         [0.3296, 0.3296, 0.3296, 0.3296],\n",
      "         [0.4295, 0.4295, 0.4295, 0.4295],\n",
      "         [0.5042, 0.5042, 0.5042, 0.5042],\n",
      "         [0.5623, 0.5623, 0.5623, 0.5623],\n",
      "         [0.6090, 0.6090, 0.6090, 0.6090],\n",
      "         [0.6475, 0.6475, 0.6475, 0.6475],\n",
      "         [0.6800, 0.6800, 0.6800, 0.6800],\n",
      "         [0.7077, 0.7077, 0.7077, 0.7077],\n",
      "         [0.7317, 0.7317, 0.7317, 0.7317]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      " final_step_hidden= torch.Size([3, 2, 4]) \n",
      " tensor([[[0.7692, 0.7692, 0.7692, 0.7692],\n",
      "         [0.7692, 0.7692, 0.7692, 0.7692]],\n",
      "\n",
      "        [[0.7272, 0.7272, 0.7272, 0.7272],\n",
      "         [0.7272, 0.7272, 0.7272, 0.7272]],\n",
      "\n",
      "        [[0.7317, 0.7317, 0.7317, 0.7317],\n",
      "         [0.7317, 0.7317, 0.7317, 0.7317]]], grad_fn=<StackBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 4\n",
    "n_layers = 3\n",
    "\n",
    "print('batch_size=',batch_tensor.shape[0])\n",
    "print('input_size =',n_chars)\n",
    "print('n_hidden =',n_hidden)\n",
    "print('n_layers = ',n_layers)\n",
    "\n",
    "\n",
    "gru_rnn = SimpleGRU(n_chars, num_layers=n_layers, bidirectional=False,hidden_dim=n_hidden,printVars=False) \n",
    "\n",
    "print('input to gru = ',batch_tensor.shape)\n",
    "emissions,h_n = gru_rnn(batch_tensor)\n",
    "\n",
    "print('\\n all_step_gru_output=',emissions.shape,'\\n',emissions)\n",
    "print('\\n final_step_hidden=',h_n.shape,'\\n',h_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Simple GRU Layer from individual cells (3 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stacked_GRU_Cells(nn.Module):\n",
    "    \"\"\" Implements a three layer GRU cell with an output linear layer back to the size of the output categories\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_dim=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gru_0 = nn.GRUCell(input_size, hidden_dim)\n",
    "        self.gru_1 = nn.GRUCell(hidden_dim, hidden_dim)\n",
    "        self.gru_2 = nn.GRUCell(hidden_dim, hidden_dim)\n",
    "        \n",
    "        #initialize biases and weights to some fixed value for testing \n",
    "        for name, param in self.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant(param, 0.3)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.constant(param, 0.3)\n",
    "                #nn.init.xavier_normal(param)\n",
    "\n",
    "    def forward(self, x, h_in):\n",
    "        h_out =  torch.zeros(h_in.size())\n",
    "        \n",
    "        h_out[0] = self.gru_0(x, h_in[0])\n",
    "        h_out[1] = self.gru_1(h_out[0], h_in[1])\n",
    "        h_out[2] = self.gru_2(h_out[1], h_in[2])\n",
    "        \n",
    "        x = h_out[2]\n",
    "        return x, h_out\n",
    "    \n",
    "    \n",
    "def forward_RNN_pass(gru_rnn, batch_tensor,hidden_dim):\n",
    "    batch_size = batch_tensor.shape[0]\n",
    "    seq_len = batch_tensor.shape[1]\n",
    "    h_init = torch.zeros(3, batch_size, hidden_dim)\n",
    "    print('Initial hidden state = ',h_init.shape)\n",
    "    \n",
    "    h = h_init\n",
    "    #To gather outputs from all timesteps\n",
    "    gru_out = torch.zeros([batch_size,seq_len,hidden_dim])\n",
    "    \n",
    "    for position in range(seq_len):\n",
    "        logits, h = gru_rnn(batch_tensor[:, position, :], h)\n",
    "        gru_out[:,position,:] = logits #store gru output from this timestep\n",
    "        \n",
    "    all_step_output = gru_out #output from final layer for all timesteps\n",
    "    final_step_hidden = h #hidden state from final timestep for all layers\n",
    "    return all_step_output, final_step_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size= 2\n",
      "input_size = 6\n",
      "n_hidden = 4\n",
      "n_layers =  3\n",
      "input to gru =  torch.Size([2, 10, 6])\n",
      "Initial hidden state =  torch.Size([3, 2, 4])\n",
      "\n",
      " all_step_gru_output= torch.Size([2, 10, 4]) \n",
      " tensor([[[0.1908, 0.1908, 0.1908, 0.1908],\n",
      "         [0.3296, 0.3296, 0.3296, 0.3296],\n",
      "         [0.4295, 0.4295, 0.4295, 0.4295],\n",
      "         [0.5042, 0.5042, 0.5042, 0.5042],\n",
      "         [0.5623, 0.5623, 0.5623, 0.5623],\n",
      "         [0.6090, 0.6090, 0.6090, 0.6090],\n",
      "         [0.6475, 0.6475, 0.6475, 0.6475],\n",
      "         [0.6800, 0.6800, 0.6800, 0.6800],\n",
      "         [0.7077, 0.7077, 0.7077, 0.7077],\n",
      "         [0.7317, 0.7317, 0.7317, 0.7317]],\n",
      "\n",
      "        [[0.1908, 0.1908, 0.1908, 0.1908],\n",
      "         [0.3296, 0.3296, 0.3296, 0.3296],\n",
      "         [0.4295, 0.4295, 0.4295, 0.4295],\n",
      "         [0.5042, 0.5042, 0.5042, 0.5042],\n",
      "         [0.5623, 0.5623, 0.5623, 0.5623],\n",
      "         [0.6090, 0.6090, 0.6090, 0.6090],\n",
      "         [0.6475, 0.6475, 0.6475, 0.6475],\n",
      "         [0.6800, 0.6800, 0.6800, 0.6800],\n",
      "         [0.7077, 0.7077, 0.7077, 0.7077],\n",
      "         [0.7317, 0.7317, 0.7317, 0.7317]]], grad_fn=<CopySlices>)\n",
      "\n",
      " final_step_hidden= torch.Size([3, 2, 4]) \n",
      " tensor([[[0.7692, 0.7692, 0.7692, 0.7692],\n",
      "         [0.7692, 0.7692, 0.7692, 0.7692]],\n",
      "\n",
      "        [[0.7272, 0.7272, 0.7272, 0.7272],\n",
      "         [0.7272, 0.7272, 0.7272, 0.7272]],\n",
      "\n",
      "        [[0.7317, 0.7317, 0.7317, 0.7317],\n",
      "         [0.7317, 0.7317, 0.7317, 0.7317]]], grad_fn=<CopySlices>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  app.launch_new_instance()\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "n_hidden = 4\n",
    "n_layers = 3 #fixed\n",
    "\n",
    "print('batch_size=',batch_tensor.shape[0])\n",
    "print('input_size =',n_chars)\n",
    "print('n_hidden =',n_hidden)\n",
    "print('n_layers = ',n_layers)\n",
    "\n",
    "gru_rnn = Stacked_GRU_Cells(n_chars, hidden_dim=n_hidden)\n",
    "\n",
    "print('input to gru = ',batch_tensor.shape)\n",
    "final_step_output, final_step_hidden = forward_RNN_pass(gru_rnn,batch_tensor,n_hidden)\n",
    "\n",
    "print('\\n all_step_gru_output=',final_step_output.shape,'\\n',final_step_output)\n",
    "print('\\n final_step_hidden=',final_step_hidden.shape,'\\n',final_step_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare these results with the output and hidden state tensors from Experiment 1\n",
    "## They should be same"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
