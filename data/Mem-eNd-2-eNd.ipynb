{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\u588401\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 28 words from testdata.txt\n"
     ]
    }
   ],
   "source": [
    "# code for Mem-N-to-N for language modelling.\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "config = {\n",
    "        'batch_size'    : 2,     # batch_size\n",
    "        'emb_dim'       : 4,     # embedding dimension for words\n",
    "        'mem_size'      : 6,     # memory size\n",
    "        'init_q'        : 0.1, \n",
    "        'n_epochs'      : 1,     # no. of epochs\n",
    "        'n_hops'        : 3,     # no. of hops in memory\n",
    "        'n_words'       : None,\n",
    "        'init_lr'       : 0.001, # initial learning rate\n",
    "        'std_dev'       : 0.05,\n",
    "        'lin_dim'       : 2,      # no. of units to have linear activation\n",
    "        'max_grad_norm' : 50     #clip gradients to this norm.\n",
    "}\n",
    "\n",
    "# read words and convert it to unique integers\n",
    "def read_data(fname, count, word2idx):\n",
    "    with open(fname) as f:\n",
    "        lines = f.readlines()\n",
    "    words = []\n",
    "    for line in lines:\n",
    "        words.extend(line.split())\n",
    "    if len(count) == 0:\n",
    "        count.append(['<eos>', 0])\n",
    "    count[0][1] += len(lines)\n",
    "    count.extend(Counter(words).most_common())\n",
    "    if len(word2idx) == 0:\n",
    "        word2idx['<eos>'] = 0\n",
    "    for word, _ in count:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "    data = list()\n",
    "    for line in lines:\n",
    "        for word in line.split():\n",
    "            index = word2idx[word]\n",
    "            data.append(index)\n",
    "        data.append(word2idx['<eos>'])\n",
    "    print(\"Read %s words from %s\" % (len(data), fname))\n",
    "    return data\n",
    "\n",
    "count = list()\n",
    "word2idx = dict()\n",
    "train_data = read_data('testdata.txt', count, word2idx)\n",
    "config['n_words'] = len(word2idx)\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "e_dim = config['emb_dim']\n",
    "l_dim = config['lin_dim']\n",
    "mem_size = config['mem_size']\n",
    "n_epochs = config['n_epochs']\n",
    "n_hops = config['n_hops']\n",
    "n_words = config['n_words'] = len(word2idx)\n",
    "current_lr = config['init_lr']\n",
    "std_dev = config['std_dev']\n",
    "init_q = config['init_q']\n",
    "max_grad_norm = config['max_grad_norm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining the tensorflow model...\n",
      "hop in memory : 0  input u_k: Tensor(\"q:0\", shape=(?, 4), dtype=float32)\n",
      "-------------\n",
      "hop in memory : 1  input u_k: Tensor(\"concat_1:0\", shape=(?, 4), dtype=float32)\n",
      "-------------\n",
      "hop in memory : 2  input u_k: Tensor(\"concat_3:0\", shape=(?, 4), dtype=float32)\n",
      "-------------\n",
      "Model specification complete...\n"
     ]
    }
   ],
   "source": [
    "print('Defining the tensorflow model...')\n",
    "\n",
    "# Define the tensorflow model. The Variable names are made to follow the paper as closely as possible.\n",
    "input_q = tf.placeholder(tf.float32, shape=[None, e_dim],name=\"q\") #the question q, will be set to all 0.1.\n",
    "input_x = tf.placeholder(tf.int32, [None, mem_size], name=\"x\") # the context word ids\n",
    "input_time = tf.placeholder(tf.int32, [None, mem_size], name=\"time\") # to lookup temporal encoding\n",
    "input_y = tf.placeholder(tf.float32, [None, n_words], name=\"target\") # id of next word to predict (target)\n",
    "\n",
    "# Matrices for input memory representation\n",
    "A = tf.Variable(tf.random_normal([n_words, e_dim], stddev=std_dev),name=\"A\")   #embedding matrix A for input memory representation\n",
    "T_A = tf.Variable(tf.random_normal([n_words, e_dim], stddev=std_dev),name=\"T_A\") #embedding matrix for temporal encoding\n",
    "# Input memory vectors : m_i = sum A_ij * x_ij + T_A_i\n",
    "x_in_A   = tf.nn.embedding_lookup(A, input_x) # embedding lookup, shape: batch_size x mem_size x e_dim \n",
    "T_A_i = tf.nn.embedding_lookup(T_A, input_time) #T_A(i), shape: batch_size x mem_size x e_dim\n",
    "mem_in = tf.add(x_in_A, T_A_i) #input memory vectors m_i, shape: batch_size x mem_size x e_dim\n",
    "\n",
    "# Matrices for output memory representation\n",
    "C = tf.Variable(tf.random_normal([n_words, e_dim], stddev=std_dev),name=\"C\") #embedding matrix C for output memory representation\n",
    "T_C = tf.Variable(tf.random_normal([n_words, e_dim], stddev=std_dev),name=\"T_C\") #embedding matrix for temporal encoding\n",
    "# Output memory vectors : c_i = sum C_ij * x_ij + T_C_i\n",
    "x_in_C   = tf.nn.embedding_lookup(C, input_x) # embedding lookup, shape: batch_size x mem_size x e_dim\n",
    "T_C_i = tf.nn.embedding_lookup(T_C, input_time) #T_C(i), shape: batch_size x mem_size x e_dim\n",
    "mem_out = tf.add(x_in_C, T_C_i) #output memory vectors c_i, shape: batch_size x mem_size x e_dim\n",
    "\n",
    "# For linear mapping of input u between hops\n",
    "Hw = tf.Variable(tf.random_normal([e_dim, e_dim], stddev=std_dev),name=\"Hw\")\n",
    "Hb = tf.Variable(tf.random_normal([e_dim], stddev=std_dev),name=\"Hb\")\n",
    "\n",
    "u_k = input_q #initialize u_k for first hop in memory, shape : batch_size x edim\n",
    "\n",
    "for k in range(n_hops): #k indexes the no. of hops in memory\n",
    "    print('hop in memory :',k,' input u_k:',u_k)\n",
    "    u_k_3d = tf.reshape(u_k, [-1, e_dim, 1]) # reshape to shape: batch_size x e_dim x 1\n",
    "    \n",
    "    # p_i = Softmax(u^T m_i) (equation 1)\n",
    "    probs = tf.nn.softmax(tf.matmul(mem_in, u_k_3d)) # shape: batch_size x mem_size x 1 \n",
    "    \n",
    "    # o = sum p_i c_i (equation 2)\n",
    "    o_k = tf.matmul(mem_out, probs, transpose_a=True) # shape: batch_size x e_dim x 1\n",
    "    o_k_2d = tf.reshape(o_k, [-1, e_dim]) # shape: batch_size x e_dim\n",
    "    \n",
    "    #apply a linear mapping H to u : u_mapped = Hw u + Hb \n",
    "    u_k_mapped = tf.add(tf.matmul(u_k,Hw),Hb)\n",
    "    \n",
    "    # u_(k+1) = u_k + o_k (equation 4)\n",
    "    u_k_next_hop = tf.add(u_k_mapped,o_k_2d)\n",
    "    \n",
    "    #apply ReLU to a slice of the units, rest of the unit activations are linear.  \n",
    "    u_k_next_hop_linear = tf.slice(u_k_next_hop, [0,0], [-1,l_dim]) #slice of u_k_next_hop to have linear activations\n",
    "    u_k_next_hop_relu = tf.slice(u_k_next_hop, [0,l_dim], [-1,e_dim - l_dim]) # remaining slice to have ReLU activations\n",
    "    u_k_next_hop_relu = tf.nn.relu(u_k_next_hop_relu)\n",
    "    u_k_next_hop = tf.concat(axis=1, values=[u_k_next_hop_linear,u_k_next_hop_relu])\n",
    "    u_k = u_k_next_hop #update u_k for the next hop in memory\n",
    "    print('-------------')\n",
    "    \n",
    "W = tf.Variable(tf.random_normal([n_words, e_dim], stddev=std_dev),name=\"W\") # final weight matrix W as in the paper.\n",
    "a_hat = tf.matmul(u_k, W, transpose_b=True)  # shape : batch_size x n_words (equation 3), the output logits.\n",
    "\n",
    "print('Model specification complete...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining the ops for model optimization ...\n"
     ]
    }
   ],
   "source": [
    "print('Defining the ops for model optimization ...')\n",
    "#Define the ops to estimate loss and optimize the above model. \n",
    "\n",
    "#change the softmax_cross_entropy_with_logits_v2 to softmax_cross_entropy_with_logits for older versions of tensorflow.\n",
    "model_loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=a_hat, labels=input_y)\n",
    "\n",
    "lr = tf.Variable(current_lr)\n",
    "opt = tf.train.GradientDescentOptimizer(lr) #optimizer\n",
    "\n",
    "params = [A, T_A, C, T_C, Hw, Hb, W] #list of Variables to optimize\n",
    "# get a List of (gradient, variable) pairs as returned by compute_gradients(...)\n",
    "grads_and_vars = opt.compute_gradients(model_loss,params)\n",
    "\n",
    "#clip the gradients using l2 norm of each variable separately, not used.\n",
    "#clipped_grads_and_vars = [(tf.clip_by_norm(gv[0], max_grad_norm), gv[1]) for gv in grads_and_vars] \n",
    "\n",
    "# Better: clip the gradients using l2 norm of the whole gradient of all variables. \n",
    "all_gradients = [gv[0] for gv in grads_and_vars]\n",
    "clipped_grads_global = tf.clip_by_global_norm(all_gradients,max_grad_norm)[0] #should be a list of clipped tensors\n",
    "clipped_grads_and_vars_global = [(clipped_grads_global[i],gv[1]) for i,gv in enumerate(grads_and_vars)]\n",
    "\n",
    "optim = opt.apply_gradients(clipped_grads_and_vars_global)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 1  batch= 1  avg_loss= 3.144498348236084\n",
      "epoch= 1  batch= 2  avg_loss= 3.140637159347534\n",
      "epoch= 1  batch= 3  avg_loss= 3.1388072967529297\n",
      "epoch= 1  batch= 4  avg_loss= 3.134405732154846\n",
      "epoch= 1  batch= 5  avg_loss= 3.1321954250335695\n",
      "epoch= 1  batch= 6  avg_loss= 3.134369413057963\n",
      "epoch= 1  batch= 7  avg_loss= 3.1347040789467946\n",
      "epoch= 1  batch= 8  avg_loss= 3.134003907442093\n",
      "epoch= 1  batch= 9  avg_loss= 3.1328342490726047\n",
      "epoch= 1  batch= 10  avg_loss= 3.1314380884170534\n",
      "epoch= 1  batch= 11  avg_loss= 3.1310913129286333\n",
      "epoch= 1  batch= 12  avg_loss= 3.1311248739560447\n",
      "epoch= 1  batch= 13  avg_loss= 3.1304379059718204\n",
      "epoch= 1  batch= 14  avg_loss= 3.1303835085460117\n",
      "epoch= 1  avg_loss= 3.1303835085460117 epoch perplexity= 22.88275359143048\n"
     ]
    }
   ],
   "source": [
    "# Define the data structures to provide data input to the model.\n",
    "q = np.ndarray([batch_size, e_dim], dtype=np.float32)\n",
    "x = np.ndarray([batch_size, mem_size])\n",
    "time = np.ndarray([batch_size, mem_size], dtype=np.int32)\n",
    "target = np.zeros([batch_size, n_words]) # each word is one-hot-encoded\n",
    "\n",
    "q.fill(init_q) # fill with all 0.1\n",
    "\n",
    "for t in range(mem_size):\n",
    "    time[:,t].fill(t)\n",
    "\n",
    "def train_one_epoch(epoch_no,sess,data):    \n",
    "    # No. of loops in one epoch\n",
    "    N = int(math.ceil(len(data) / batch_size))\n",
    "    total_loss = 0\n",
    "    \n",
    "    for idx in range(1,N+1):\n",
    "        target.fill(0)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            t_idx = random.randrange(mem_size, len(data)) #choose a word index beyond mem_size. \n",
    "            target[b][data[t_idx]] = 1 #set the word at the chosen index to be the target word to predict.\n",
    "            # the context\n",
    "            x[b] = data[t_idx - mem_size : t_idx] #set to the mem_size words preceeding the target word.\n",
    "            \n",
    "        f_dict = {\n",
    "            input_q: q, \n",
    "            input_x: x, \n",
    "            input_time: time, \n",
    "            input_y: target \n",
    "        }\n",
    "        _, batch_loss = sess.run([optim,model_loss],feed_dict=f_dict)\n",
    "        total_loss += np.sum(batch_loss)\n",
    "        cost = total_loss/(idx*batch_size)\n",
    "        print('epoch=',epoch_no,' batch=',idx,' avg_loss=',cost)\n",
    "        \n",
    "    cost = total_loss/(N*batch_size)    \n",
    "    print('epoch=',epoch_no,' avg_loss=',cost, \"epoch perplexity=\",np.exp(cost))    \n",
    "         \n",
    "# Define session to run the model with data            \n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()    \n",
    "    train_one_epoch(1,sess,train_data)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
